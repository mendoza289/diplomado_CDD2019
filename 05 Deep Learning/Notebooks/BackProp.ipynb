{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/diplomado_CDD2019/blob/master/05%20Deep%20Learning/Notebooks/BackProp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:50px;\" align=\"left\"> <img align=\"left\" width=\"30%\" src=\"../img/cerebro_1.jpg\"/>    \n",
    "    Redes Neuronales Artificiales - Algoritmo BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para subir carpetas a Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install subversion\n",
    "# !svn checkout \"https://github.com/jhermosillo/diplomado_CDD2019/trunk/05%20Deep%20Learning/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Datos de entrenamiento X, y\n",
    "data = loadmat('Data/datosMNIST')\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# Estos datos fueron creados en MATLAB donde no hay índice 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# tamaño del conjunto de instancias\n",
    "m = y.size\n",
    "\n",
    "# indices de una permutación aleatoria de instancias \n",
    "# para visualizarlas\n",
    "indices = np.random.permutation(m)\n",
    "\n",
    "# Elegimos 100 puntos al azar para desplegar\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :].reshape((10,10,-1))\n",
    "\n",
    "# visualización de los datos\n",
    "fig, axarr = plt.subplots(10,10,figsize=(10,10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axarr[i,j].imshow(sel[i,j].reshape((20,20), order = 'F'))          \n",
    "        axarr[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros para el ejercicio\n",
    "* Al entrenar redes neuronales, es importante inicializar los parámetros con valores aleatorios para evitar simetrías. \n",
    "* Una estrategia efectiva es elegir valores iniciales aleatorios para  $\\Theta^{(l)}$ unifórmemente distribuídos en el rango $[-\\epsilon_{init},\\epsilon_{init}]$. \n",
    "* Para elegir $\\epsilon_{init}$ una forma efectiva es inicializarlo con base en el número de neuronas en la red. \n",
    "* Una buena elección de  $\\epsilon_{init}$ es $$\\epsilon_{init}=\\frac{\\sqrt{6}}{\\sqrt{L_{in}+L_{out}}}$$ donde $L_{in}=s_l$ and $L_{out}=s_{l+1}$ son el número de neuronas en las capas adyacentes a $\\Theta^{(l)}$. \n",
    "* Se sugiere usar $\\epsilon_{init}=0.12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon = 0.12\n",
    "    return np.random.rand(L_out, L_in+1) * 2 * epsilon - epsilon\n",
    "\n",
    "# Parámetros\n",
    "neuronas_de_entrada  = 400  # Imágenes de entrada 20x20\n",
    "neuronas_ocultas = 25   # 25 neuronas coultas\n",
    "etiquetas = 10          # 10 etiquetas, clases de 0 a 9\n",
    "\n",
    "Theta1 = randInitializeWeights(neuronas_de_entrada, neuronas_ocultas)\n",
    "Theta2 = randInitializeWeights(neuronas_ocultas, etiquetas)\n",
    "\n",
    "lmbda = 1\n",
    "# desenrrollamos los parámetros para la función de optimización\n",
    "rna_params = np.hstack((Theta1.ravel(order='F'), Theta2.ravel(order='F')))    #unroll parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Theta1.shape)\n",
    "print(Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal que vamos a implementar es la siguiente:\n",
    "\n",
    "<img align=\"left\" width=\"50%\" src=\"../img/redneuronal.jpg\"/> \n",
    "\n",
    "Tiene 3 capas: una capa de entrada, una capa oculta y una capa de salida. Recuerda que nuestras entradas son valores de pixeles de imágenes de dígitos. Dado que las imágenes tienen un tamaño de 20 × 20, esto nos da 400 neuronas de capa de entrada (excluyendo la neurona de polarización adicional que siempre genera +1). Como siempre, los datos de entrenamiento se cargarán en las variables $X$,$y$.\n",
    "\n",
    "Se te ha proporcionado un conjunto de parámetros de la red ($\\Theta^{(1)}$, $\\Theta^{(2)}$) ya entrenados. Estos están almacenados en `pesosFW.mat`. La siguiente celda carga esos parámetros en `Theta1` y` Theta2`. Los parámetros tienen dimensiones para una red neuronal con 25 neuronas en la segunda capa y 10 neuronas de salida (correspondientes a las clases de 10 dígitos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Costo\n",
    "\n",
    "* Primero implementaremos la función de costo seguida del gradiente para la red neuronal (para la que usaremos el algoritmo BackPropagation). \n",
    "\n",
    "* La función de costo con regularización L2 para nuestro problema es:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\left[ -y^{(i)}_k \\log\\left(\\left(h_\\theta\\left( x^{(i)} \\right)\\right)_k \\right) - \\left( 1 - y^{(i)}_k\\right) \\log \\left( 1 - \\left(h_\\theta\\left( x^{(i)} \\right)\\right)_k \\right) \\right]+\\frac{\\lambda}{2m}\\Big[\\sum_{j=1}^{25} \\sum_{k=1}^{400}(\\Theta^{(1)}_{j,k})^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25}(\\Theta^{(2)}_{j,k})^2 \\Big]$$\n",
    "\n",
    "donde $K$ es el número total de etiquetas (10 en este caso), $\\left(h_\\theta\\left( x^{(i)} \\right)\\right)_k = a^{(i)}_k$ es la activación de la neurona $k$ y $y^{(i)}_k$ es $1$ o $0$. \n",
    "\n",
    "* Recuerda que:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y^{(i)} &= \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\n",
    "         \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           0\n",
    "         \\end{bmatrix}, \\cdots,\n",
    "         \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           1\n",
    "         \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnaCosto(rna_params, entradas, ocultas, etiquetas, X, y, lmbda):\n",
    "    theta1 = np.reshape(rna_params[:ocultas*(entradas+1)], (ocultas, entradas+1), 'F')\n",
    "    theta2 = np.reshape(rna_params[ocultas*(entradas+1):], (etiquetas, ocultas+1), 'F')\n",
    "    \n",
    "    m = len(y)\n",
    "    ones = np.ones((m,1))\n",
    "    \"\"\"Debes crear las siguientes variables correctamente\"\"\"\n",
    "    a1 = 0\n",
    "    a2 = 0\n",
    "    h = 0\n",
    "    \"\"\"----------------TU CODIGO AQUI--------------\"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"--------------------------------------------\"\"\"\n",
    "    \n",
    "    y_d = pd.get_dummies(y.flatten())    \n",
    "        \n",
    "    temp1 = np.multiply(y_d, np.log(h))\n",
    "    temp2 = np.multiply(1-y_d, np.log(1-h))\n",
    "    temp3 = np.sum(temp1 + temp2)\n",
    "    \n",
    "    sum1 = np.sum(np.sum(np.power(theta1[:,1:],2), axis = 1))\n",
    "    sum2 = np.sum(np.sum(np.power(theta2[:,1:],2), axis = 1))\n",
    "    \n",
    "    return np.sum(temp3 / (-m)) + (sum1 + sum2) * lmbda / (2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba el código\n",
    "* Debe ser circa 7.0 (+/- 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J=rnaCosto(rna_params, neuronas_de_entrada, neuronas_ocultas, etiquetas, X, y, lmbda)\n",
    "print('J=',J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Gradiente\n",
    "\n",
    "* El gradiente de la función sigmoide es:\n",
    "\n",
    "$$ g'(z)=\\frac{d}{dz}g(z)=g(z)(1-g(z))$$\n",
    "\n",
    "donde $$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGrad(z):\n",
    "    return np.multiply(sigmoide(z), 1-sigmoide(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation\n",
    "\n",
    "* Recuerda que la intuición detrás del algoritmo de retropropagación es la siguiente. Dado un ejemplo de entrenamiento  \n",
    " $\\big(x^{(t)},y^{(t)}\\big)$  ejecutamos primero un \"pase hacia adelante\" para calcular todas las activaciones en la red, incluido el valor de salida del hipótesis $h_\\Theta(x)$. Luego, para cada nodo $j$ en la capa $l$, nos gustaría calcular el \"término de error\" $\\delta_j^{(l)}$ que mide cuánto ese nodo fue \"responsable\" de cualquier error en nuestra salida . \n",
    "\n",
    "* En resumen, hacemos lo siguiente recorriendo cada instancia de entrenamiento:\n",
    " 0. Establece los valores de la capa de entrada $(a^{(1)})$ a la instancia de entrenamiento $i$-ésima $x^{(i)}$.\n",
    " 1. Realiza una predición directa, calculando las activaciones $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$. Recuerda agregar un término $+1$ para incluir el sesgo en las capas 2 y 3.\n",
    " 2. Calcula el término de error $\\delta_k^{(3)} $ para cada unidad de salida como $\\delta_k^{(3)} = (a_k^{(3)} - y_k)$, $y_k \\in \\{0,1\\}$.\n",
    " 3. Para la capa oculta $l = 2$, establece $$\\delta^{(2)} = \\big(\\Theta^{((2)}\\big)^T \\delta^{(3) }.*g'\\big(z^{(2)}\\big)$$ donde $.* $ indica la multiplicación elemento a elemento.\n",
    " 4. Acumula los gradientes usando $\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l + 1)} \\big(a^{(l)}\\big)^T$. Ten en cuenta que debes omitir $\\delta_0^{(2)}$.\n",
    " 5. Obtén los gradientes (no regularizados) para la función de costo de la red neuronal dividiendo los gradientes acumulados (del paso 4) entre m.\n",
    " 6. Agrega los términos de regularización a los gradientes usando\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) &= D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)} & \\textrm{ for } j =0 \\\\\n",
    "    \\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta) &= D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)} + \\frac{\\lambda}{m}\\Theta_{ij}^{(l)} & \\textrm{ for } j \\ge 1     \n",
    "\\end{align}\n",
    "$$\n",
    "* Ten en cuenta que no debes regularizar la primera columna de $\\Theta^{(l)}$ que se usa para el término de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnaGrad(rna_params, entradas, ocultas, etiquetas, X, y, lmbda):\n",
    "    \n",
    "    theta1_inicial = np.reshape(rna_params[:ocultas*(entradas+1)], (ocultas, entradas+1), 'F')\n",
    "    theta2_inicial = np.reshape(rna_params[ocultas*(entradas+1):], (etiquetas, ocultas+1), 'F')\n",
    "    \n",
    "    #convert categorical classes into one-hot vectors data frame\n",
    "    y_d = pd.get_dummies(y.flatten())   \n",
    "    \n",
    "    m = len(y)\n",
    "    #inicialización de los errores de gradiente\n",
    "    delta1 = np.zeros(theta1_inicial.shape)\n",
    "    delta2 = np.zeros(theta2_inicial.shape)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        ones = np.ones(1)\n",
    "        \"\"\"----------------TU CODIGO AQUI--------------\"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\"--------------------------------------------\"\"\"\n",
    "        #CODIGO para delta_3 y delta_2\n",
    "        d3 = a3 - y_d.iloc[i,:][np.newaxis,:]\n",
    "        z2 = np.hstack((ones, z2))\n",
    "        d2 = np.multiply(theta2_inicial.T @ d3.T, sigmoidGrad(z2).T[:,np.newaxis])\n",
    "        \n",
    "        \"\"\"Debes actualizar las siguientes variables correctamente\"\"\"\n",
    "        \"\"\"----------------TU CODIGO AQUI--------------\"\"\"\n",
    "\n",
    "        \n",
    "        \"\"\"--------------------------------------------\"\"\"\n",
    "        \n",
    "    delta1 /= m\n",
    "    delta2 /= m\n",
    "    #print(delta1.shape, delta2.shape)\n",
    "    delta1[:,1:] = delta1[:,1:] + theta1_inicial[:,1:] * lmbda / m\n",
    "    delta2[:,1:] = delta2[:,1:] + theta2_inicial[:,1:] * lmbda / m\n",
    "        \n",
    "    return np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_backprop_Params = rnaGrad(rna_params, neuronas_de_entrada, neuronas_ocultas, etiquetas, X, y, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checaGradiente(rna_initial_params,rna_backprop_Params,entradas, ocultas, etiquetas,myX,myy,mylambda=0.):\n",
    "    myeps = 0.0001\n",
    "    flattened = rna_initial_params\n",
    "    flattenedDs = rna_backprop_Params\n",
    "    n_elems = len(flattened) \n",
    "    #Toma 10 elementos aleatorios, calcula el gradiente numérico, comprara contra las respectivas D's\n",
    "    for i in range(10):\n",
    "        x = int(np.random.rand()*n_elems)\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = myeps\n",
    "\n",
    "        cost_high = rnaCosto(flattened + epsvec.flatten(),entradas, ocultas, etiquetas,myX,myy,mylambda)\n",
    "        cost_low  = rnaCosto(flattened - epsvec.flatten(),entradas, ocultas, etiquetas,myX,myy,mylambda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*myeps)\n",
    "        print(\"Elemento: {0}. Gradiente Numérico = {1:.9f}. Gradiente BackProp  = {2:.9f}.\".format(x,mygrad,flattenedDs[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checaGradiente(rna_params,rna_backprop_Params,neuronas_de_entrada, neuronas_ocultas, etiquetas,X,y,lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = opt.fmin_cg(maxiter = 50, f = rnaCosto, x0 = rna_params, fprime = rnaGrad, \\\n",
    "                        args = (neuronas_de_entrada, neuronas_ocultas, etiquetas, X, y.flatten(), lmbda))\n",
    "\n",
    "theta1_opt = np.reshape(theta_opt[:neuronas_ocultas*(neuronas_de_entrada+1)], (neuronas_ocultas, neuronas_de_entrada+1), 'F')\n",
    "theta2_opt = np.reshape(theta_opt[neuronas_ocultas*(neuronas_de_entrada+1):], (etiquetas, neuronas_ocultas+1), 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta1, theta2, X, y):\n",
    "    m = len(y)\n",
    "    ones = np.ones((m,1))\n",
    "    a1 = np.hstack((ones, X))\n",
    "    a2 = sigmoide(a1 @ theta1.T)\n",
    "    a2 = np.hstack((ones, a2))\n",
    "    h = sigmoide(a2 @ theta2.T)\n",
    "    return np.argmax(h, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(theta1_opt, theta2_opt, X, y)\n",
    "print('Exactitud: ',np.mean(pred == y.flatten()) * 100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
