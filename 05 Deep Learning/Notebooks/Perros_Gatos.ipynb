{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jhermosillo/diplomado_CDD2019/blob/master/05%20Deep%20Learning/Notebooks/Perros_Gatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:50px;\" align=\"left\"> <img align=\"left\" width=\"100%\" src=\"../img/data_science_rec.jpg\"/> <br> <br> Clasificación con Redes Neuronales Convolucionales (CNN)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "table, td, th {  \n",
    "  border: 1px solid #ddd;\n",
    "  text-align: left;\n",
    "}\n",
    "|  <img src=\"../img/data_science.jpg\" width=\"300\"/> |   <font color='midnightblue'>Diplomado en <br> Ciencia de Datos <br> con Python</font>|\n",
    "|:-:|:-|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#128214; <u>Referencias bibliográficas y sitios de interés</u>:\n",
    "* Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press.\n",
    "* [Keras an API for Tensorflow](https://keras.io/getting_started/)\n",
    "* [Tensorflow: end-to-end open source machine learning platform](https://www.tensorflow.org/)\n",
    "* [Deep Learning with Keras and TensorFlow](https://www2.mpia-hd.mpg.de/homes/dgoulier/MLClasses/Course%20-%20Deep%20Learning%20with%20Keras%20and%20TensorFlow%20-%20Part%201.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WG3DcGuhdyaT"
   },
   "source": [
    "* El conjunto de datos de Dogs vs Cats fue publicado por Kaggle.com como parte de una competencia de visión computacional a fines de 2013, cuando las CNNs no eran muy comunes.\n",
    "\n",
    "* Se puede descargar el dataset original en: https://www.kaggle.com/c/dogs-vs-cats/data (se necesita crear una cuenta de Kaggle).\n",
    "\n",
    "* El conjunto de datos original para entrenamiento contiene 25,000 imágenes de perros y gatos (12,500 de cada clase) y tiene un tamaño de 543 MB. \n",
    "\n",
    "* Aquí usaremos un subconjunto más pequeño que contiene: un conjunto de entrenamiento con 1,000 imágenes de cada clase, un conjunto de validación con 500 imágenes de cada clase y finalmente, un conjunto de pruebas con 500 imágenes de cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificación del entorno de ejecución (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11102,
     "status": "ok",
     "timestamp": 1592516407484,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjDs06OkjiU7XO6BThXLIL36RXS1QwwKJrRW3J-mw=s64",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "ln6imMSY8vLe",
    "outputId": "508d00bb-74bb-4a7b-ccaa-0a971037fbbf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para subir carpetas a Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install subversion\n",
    "!svn checkout \"https://github.com/jhermosillo/diplomado_CDD2019/trunk/05%20Deep%20Learning/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyibPS7PQz3t"
   },
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1798,
     "status": "ok",
     "timestamp": 1561413045118,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "6G3WswyR8vho",
    "outputId": "3c7a0920-c88a-4e7f-9a64-7ef6cdacda84"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "file_name = 'Data/cnn_perros_gatos.zip'\n",
    "\n",
    "with ZipFile(file_name, 'r') as myzip:\n",
    "    myzip.extractall()\n",
    "    print('Listo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDWZBWGX8vqs"
   },
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1561413058978,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "JMypQXip8vzN",
    "outputId": "b64f1229-e8fd-4616-c356-d2a0fa119ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imágenes de gato para entrenamiento: 1000\n"
     ]
    }
   ],
   "source": [
    "train_dogs = 'cnn_perros_gatos/train/dogs'\n",
    "print('Para entrenamiento:')\n",
    "print('{} Perros.'.format(len(os.listdir(train_dogs))))\n",
    "train_cats = 'cnn_perros_gatos/train/cats'\n",
    "print('{} Gatos.'.format(len(os.listdir(train_cats))))\n",
    "print('\\nPara validación:')\n",
    "validation_dogs = 'cnn_perros_gatos/validation/dogs'\n",
    "print('{} Perros.'.format(len(os.listdir(validation_dogs))))\n",
    "validation_cats = 'cnn_perros_gatos/validation/cats'\n",
    "print('{} Gatos.'.format(len(os.listdir(validation_cats))))\n",
    "print('\\nPara prueba:')\n",
    "test_dogs = 'cnn_perros_gatos/test/dogs'\n",
    "print('{} Perros.'.format(len(os.listdir(test_dogs))))\n",
    "test_cats = 'cnn_perros_gatos/test/cats'\n",
    "print('{} Gatos.'.format(len(os.listdir(test_cats))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJGFJS-qNLJc"
   },
   "outputs": [],
   "source": [
    "train_dir = 'cnn_perros_gatos/train'\n",
    "validation_dir = 'cnn_perros_gatos/validation'\n",
    "train_cats_dir = 'cnn_perros_gatos/train/cats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yySANB-NNr4w"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', \n",
    "                           input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1561413094724,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "FSL8Y3n7rLtL",
    "outputId": "3dda6473-5e28-454b-ab68-691b39a6fa11"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pbnp2bbL9jes"
   },
   "source": [
    "* NOTA que comenzamos con imágenes de tamaño 150 x 150 (una elección de tamaño arbitraria) y terminamos con mapas de características de tamaño 7 x 7 justo antes de la capa de *flatten*.\n",
    "* En realidad las imágenes de entrada tienen tamaños diversos (desconocidos), pero afortunadamente Keras nos puede ayudar a pre-procesarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94vCX8FgrPgU"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_Zxc_HG1MuG"
   },
   "source": [
    "# Preprocesamiento de datos\n",
    "\n",
    "\n",
    "Los datos deben formatearse en tensores de punto flotante preprocesados adecuadamente antes de que se introduzcan en la red. En este momento, nuestros datos se encuentran almacenados como archivos JPEG, por lo que los pasos para que puedan ser introducidos en nuestra red son:\n",
    "\n",
    "* Leer los archivos de imagen.\n",
    "\n",
    "* Decodificar el contenido JPEG a cuadrículas de píxeles RBG.\n",
    "\n",
    "* Convertirlos en tensores de punto flotante.\n",
    "\n",
    "* Volver a escalar los valores de píxeles (entre 0 y 255) al intervalo $[0, 1]$ (las redes neuronales prefieren tratar con valores de entrada pequeños).\n",
    "\n",
    "Afortunadamente, Keras tiene herramientas para encargarse de estos pasos automáticamente. Keras tiene un módulo con herramientas de ayuda para procesamiento de imágenes, ubicado en **keras.preprocessing.image**. En particular, contiene la clase **ImageDataGenerator**, que permite configurar rápidamente los generadores de Python que pueden convertir automáticamente los archivos de imagen en disco en *batches* de tensores preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1561413202802,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "4LAZtTFHvkvZ",
    "outputId": "0885bda4-5de5-4334-dde5-a756ed94e305"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Todas las imágenes serán reescaladas por 1./255.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Este es el directorio de origen\n",
    "        train_dir,\n",
    "        # Todas las imágenes se redimensionarán a 150 x 150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Ya que usamos la función de pérdida de entropía cruzada binaria, \n",
    "        # necesitamos etiquetas binarias\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHKu7G_y4fPC"
   },
   "source": [
    "* Vamos a revisar la salida de uno de estos generadores: produce batches de imágenes de 150 x 150 RGB (con la forma (20, 150, 150, 3)) y etiquetas binarias (con la forma (20,)). 20 es el número de muestras en cada batch (el tamaño del batch). \n",
    "* Como el generador genera estos batches de forma indefinida (i.e. recorre sin fin las imágenes presentes en la carpeta que se le indicó), se necesita romper el loop de iteración en algún punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1561413222777,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "51xbh9LMv0yQ",
    "outputId": "b88e5b98-6b0c-4978-e780-51cad98e40fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del batch de imágenes: (20, 150, 150, 3)\n",
      "Dimensiones del batch de las etiquetas: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('Dimensiones del batch de imágenes:', data_batch.shape)\n",
    "    print('Dimensiones del batch de las etiquetas:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LevQH8ih4ek_"
   },
   "source": [
    "* Vamos a proceder a entrenar nuestro modelo con los datos usando el generador. Debido a que los datos se generan infinitamente, el generador necesita saber cuántas muestras extraer antes de declarar una época finalizada. Esta es la función del argumento **steps_per_epoch** \n",
    "\n",
    "* En este caso, **steps_per_epoch** corresponde al número de batches que requiere el generador para leer el conjunto de datos completo. Sólo después de haber solicitado este número de batches, el proceso de ajuste de nuestro modelo pasará a la siguiente época. **steps_per_epoch** corresponde a el número de pasos de descenso del gradiente. En nuestro caso, cada batch tiene un tamaño de 20 muestras, por lo que tomará 100 pasos (batches) hasta que cubramos las 2,000 muestras de nuestra base de datos.\n",
    "\n",
    "* Como siempre, uno puede pasar un argumento llamado **validation_data**. Es importante destacar que este argumento puede ser un generador de datos en sí mismo, pero también podría ser una tupla de arreglos Numpy. Si se pasa un generador como **validation_data**, entonces se espera que este generador produzca batches de datos de validación sin fin, y por lo tanto también se debe especificar el argumento **validation_steps**, que le dice al proceso cuántos batches debe extraer del generador de validación para su evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 286627,
     "status": "ok",
     "timestamp": 1561413621756,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "CdjhzcgzwQ-T",
    "outputId": "cd03d0e1-80b6-40e6-cd68-225be5ca33ca"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFnMW2EbyyqD"
   },
   "outputs": [],
   "source": [
    "model.save('cnn_perros_gatos_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1561414205963,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "lt6kRFIYzjPb",
    "outputId": "ad24188d-f211-4c4b-c885-de6e7c82d49a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Error de entrenamiento')\n",
    "plt.plot(epochs, val_loss, 'r', label='Error de validacion')\n",
    "plt.title('Error de entrenamiento y validacion')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, label='Accuracy de entrenamiento')\n",
    "plt.plot(epochs, val_acc, label='Accuracy de validación')\n",
    "plt.title('Accuracy de entrenamiento y validación')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXzBZmaj_V2_"
   },
   "source": [
    "# Aumento de Datos\n",
    "\n",
    "* El efecto de sobreajuste ocurre cuando se tienen muy pocas muestras de las que aprender, lo que nos impide entrenar un modelo capaz de generalizar a nuevos datos. Si tuviesemos datos infinitos, nuestro modelo estaría expuesto a todos los aspectos posibles de la distribución de datos en cuestión y nunca se sobreajustaría nuestro modelo. \n",
    "\n",
    "* El aumento de datos adopta el enfoque de generar más datos de entrenamiento a partir de muestras de entrenamiento existentes, al \"aumentar\" las muestras a través de una serie de transformaciones aleatorias que producen imágenes de apariencia creíble. El objetivo es que durante el tiempo de entrenamiento, nuestro modelo nunca vea exactamente la misma imagen dos veces. Esto ayuda a que el modelo se exponga a más aspectos de los datos y generalice mejor.\n",
    "\n",
    "* En Keras, esto se puede hacer configurando una serie de transformaciones aleatorias que se realizarán en las imágenes leídas por nuestra instancia de ImageDataGenerator.\n",
    "\n",
    "* Vamos a comenzar por aumentar una imagen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3_qyda81Q81"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U48tZWbBB2TM"
   },
   "source": [
    "Las opciones anteriores son solo algunas de las opciones disponibles.\n",
    "\n",
    "* **rotation_range** es un valor en grados (0-180), un rango dentro del cual girar las imágenes de forma aleatoria.\n",
    "\n",
    "* **width_shift** y **height_shift** son rangos expresados como una fracción del ancho o altura total de la imagen, dentro de los cuales se pueden trasladar vertical u horizontalmente de forma aleatoria a las imágenes.\n",
    "\n",
    "* **shear_range** aplica aleatoriamente transformaciones de corte.\n",
    "\n",
    "* **zoom_range** aplica acercamientos aleatorios dentro de las imágenes.\n",
    "\n",
    "* **horizontal_flip** Voltea de forma aleatoria la mitad en las imágenes horizontalmente.\n",
    "\n",
    "* **fill_mode** es la estrategia utilizada para rellenar píxeles creados, que pueden aparecer después de una rotación o un cambio de ancho / altura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1975,
     "status": "ok",
     "timestamp": 1561414232073,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "cbyqp-q61cbr",
    "outputId": "7e7f56c1-7feb-4a62-e73e-d08c40d21394"
   },
   "outputs": [],
   "source": [
    "# Módulo con métodos para preprocesamiento de imágenes.\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# Escogemos una imagen para aplicar el \"aumentado de datos\"\n",
    "img_path = fnames[5]\n",
    "\n",
    "# Leemos la imagen y la redimensionamos.\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "# Convertimos la imagen en un arreglo de Numpy de dimensiones (150, 150, 3)\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Redimensionamos el arreglo a (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# El comando .flow () genera batches de imágenes transformadas aleatoriamente\n",
    "# Con el \"for\" de abajo estaremos en un loop indefinidamente, \n",
    "# Vamos a necesitar 'romper' el loop en algún momento\n",
    "\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teFUV-JSFKZH"
   },
   "source": [
    "* Si entrenamos una nueva red neuronal utilizando esta configuración de aumento de datos, nuestra red nunca verá dos veces la misma entrada, pues a cada nueva imagen se le aplica transformaciones aleatorias dentro de ciertos rangos. \n",
    "\n",
    "* Sin embargo, las entradas que ves están aún muy interrelacionadas, ya que provienen de un pequeño número de imágenes originales: **no podemos producir nueva información, sólo podemos mezclar la información existente**. \n",
    "\n",
    "* Dado que esto podría no ser suficiente para librarnos del sobreajuste. Para mitigarlo aún más, también agregaremos una capa de Dropout a nuestro modelo, justo antes de la etapa del clasificador densamente conectado (fully-connected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYgaCOd2R4HU"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1561414295570,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "RbgrTc9UV5i-",
    "outputId": "409fdbe6-862b-4aec-dd40-bacf519e42aa"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6bwra-5TfZ3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2476502,
     "status": "ok",
     "timestamp": 1561416944949,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "8hc8gia_TwWn",
    "outputId": "336973fc-9d84-4e1f-8ea1-3e34e84d149e"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Debemos notar que los datos del conjunto validación no son sometidos \n",
    "# al aumentado de datos\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # Esta es la carpeta de origen\n",
    "        train_dir,\n",
    "        # Todas las imágenes se redimensionarán a 150 x 150 \n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Dado que usamos la función de entropía cruzada binaria, \n",
    "        # necesitamos etiquetas binarias\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Observación, para mejores resultados entrenar con epochs = 100\n",
    "# Por cuestiones de tiempo se entrena con epochs = 30\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5VV-0J0UTGN"
   },
   "outputs": [],
   "source": [
    "# Descomentar la siguiente linea si entrenaste con epochs = 100\n",
    "# model.save('cnn_perros_gatos_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1561416972689,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "_pYHbXOlUXPx",
    "outputId": "51ef0f3d-8c7a-43fb-f213-a55b75c7f8a7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, loss, label='Error de entrenamiento')\n",
    "plt.plot(epochs, val_loss, label='Error de validacion')\n",
    "plt.title('Error de entrenamiento y validacion')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc,label='Accuracy de entrenamiento')\n",
    "plt.plot(epochs, val_acc,label='Accuracy de validacion')\n",
    "plt.title('Accuracy de entrenamiento y validación')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_-OWUThLg-f"
   },
   "source": [
    "# ¿Qué pasaría si entrenas con 100 épocas?\n",
    "\n",
    "Con epochs = 100 se obtendrían los siguientes resultados:\n",
    "\n",
    "![](https://drive.google.com/uc?id=11R0eVCGKX3Av7fqDEmpqJ5WQ3XtyOMpS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKDZA5MnVnt3"
   },
   "source": [
    "# Parte II - Visualizando los mapas de características de una red neuronal convolucional\n",
    "\n",
    "Algunos señalan que los modelos de aprendizaje profundo funcionan como \"cajas negras\", pues aprenden representaciones que son difíciles de extraer y presentar de una forma legible para el ser humano. \n",
    "\n",
    "Si bien esto es parcialmente cierto para algunos tipos de modelos de aprendizaje profundo, definitivamente no lo es para las redes convolucionales (*CNN*). Las representaciones aprendidas por las redes convolucionales son altamente susceptibles de visualización, en gran parte porque son representaciones de conceptos visuales. Desde 2013, se ha desarrollado una amplia gama de técnicas para visualizar e interpretar estas representaciones. No exploraremos todas ellas, pero mencionaremos tres de las más accesibles y útiles:\n",
    "\n",
    "\n",
    "\n",
    "*   **Visualización de las salidas intermedias de una *CNN*  (\"activaciones intermedias\")**. Este método es útil para entender cómo las capas sucesivas de una red convolucional transforman su entrada y para obtener una noción de la función de los filtros individuales en una red convolucional .\n",
    "\n",
    "*   **Visualización de los  filtros en una CNN**. Este método es útil para entender con precisión a qué patrón o concepto visual es receptivo cada filtro en una red convolucional.\n",
    "\n",
    "*   **Visualización de los mapas de calor de activación por clase en una imagen**. Este método es útil para entender qué parte de una imagen se identificó como perteneciente a una clase determinada y, por lo tanto, permite localizar objetos en imágenes.\n",
    "\n",
    "En este ejercicio, abordaremos únicamente el primer método, la visualización de las activaciones intermedias o mapas de características. Para ello, usaremos la pequeña CNN que entrenamos desde cero en el problema de clasificación de perros y gatos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1561417000667,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "cfXwuYwPda0Q",
    "outputId": "434a20d2-19ff-498a-ad42-b09d48e6bc62"
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('Data/cnn_perros_gatos_2.h5')\n",
    "# Para recordar nuestro modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qczIqCMEzRcb"
   },
   "source": [
    "### &#9998; Vamos a seleccionar una imagen de entrada, puede ser cualquier imagen del **conjunto de test**.\n",
    "\n",
    "* En este ejemplo, usaremos la imagen de un gato. Por ser del conjunto de test, no forma parte de las imágenes sobre las que se entrenó la red:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1561417007948,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "D2CcwGp7dcMD",
    "outputId": "06d6d4c4-2b99-48fd-d275-ee9ec43e747e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = 'Data/cnn_perros_gatos/test/cats/cat.1700.jpg'\n",
    "# img_path = 'cnn_perros_gatos/test/dogs/dog.1517.jpg'\n",
    "\n",
    "# Preprocesamos la imagen en un tensor 4D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "\n",
    "# Debemos recordar que el modelo fue entrenado con imagenes de entrada\n",
    "# que fueron preprocesadas de la siguiente manera:\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Debemos ver que su forma es de (1, 150, 150, 3)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjD5E3W05kjX"
   },
   "source": [
    "* Vamos a desplegar nuestra imagen de gato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1561417013605,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "iRaoI0vSdfcF",
    "outputId": "1324dfcf-a899-4a9a-cc82-543289361504"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZH5ebeoL56FH"
   },
   "source": [
    "* Para extraer los mapas de características que queremos visualizar, crearemos un modelo de Keras que toma lotes ó *batches* de imágenes como entrada y genera las activaciones de todas las capas de convolución y *pooling*. \n",
    "* Para ello, utilizaremos la clase de Keras **Model**. Un **model** se instancia mediante dos argumentos: un tensor de entrada (o lista de tensores de entrada) y un tensor de salida (o lista de tensores de salida). La clase resultante es un modelo de Keras, igual que los modelos secuenciales (Sequential models) que ya estudiamos, que mapea las entradas especificadas a las salidas especificadas. Lo que distingue a la clase **Model** es que permite modelos con múltiples salidas, a diferencia de **Sequential**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiDTipp_diyQ"
   },
   "outputs": [],
   "source": [
    "# Extraemos las salidas de las 8 capas superiores:\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "# Creamos un modelo que devolverá estas salidas, dada la entrada al modelo:\n",
    "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bB4r9KkU9IEy"
   },
   "source": [
    "* Cuando se introduce una imagen como entrada a la red, este modelo devuelve los valores de las activaciones de las capas del modelo original. Hasta antes de esta sección del ejercicio, el modelo que se presentó sólo tenía exactamente una entrada y una salida. Ahora estamos introduciendo el concepto de un modelo con múltiples salidas.\n",
    "\n",
    "* En el caso general, un modelo podría tener cualquier número de entradas y salidas. Este último modelo tiene una entrada y 8 salidas, una salida por capa de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwuC7FEWdl8y"
   },
   "outputs": [],
   "source": [
    "# Esto devolverá una lista de arreglos de Numpy:\n",
    "# un arreglo por capa de activación\n",
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4--_UfiCP1O"
   },
   "source": [
    "Por ejemplo, vamos a imprimir las dimensiones  del mapa de características/activaciones de la primer capa convolucional para la imagen del gato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1561417030538,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "MnYfbrz1doar",
    "outputId": "b3841595-eb08-4ce8-cab5-b602792cd4da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 148, 148, 32)\n"
     ]
    }
   ],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z460Fyq3TWJm"
   },
   "source": [
    "Es un mapa de características con una dimensión de 148 x 148 con 32 canales o profundidad. \n",
    "\n",
    "Vamos a visualizar el 3er canal de ese mapa de características.\n",
    "\n",
    "Nota: En Python, los arreglos comienzan en 0, por lo que para acceder al canal 1, lo haríamos con el índice 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1561417034713,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "lAMBCCJxdqj1",
    "outputId": "d495c14b-3da8-4d47-c68a-3888883f0edf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 2], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3ICdttbWbPW"
   },
   "source": [
    "### &#9998; Probemos algún otro canal, pero debemos tener en cuenta que los canales que tenga uno pueden variar de los que tiene cualquier otro, ya que los filtros que en específico aprende la red en las capas convolucionales no son determinísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1561417039989,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "d1pjytgcdu_B",
    "outputId": "a035f02c-d0f1-4f1d-b2ad-e7ed8fb517f3"
   },
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhiUH3GhX29w"
   },
   "source": [
    "Finalmente, vamos a desplegar un gráfico completo de todas las activaciones en la red. En otras palabras, vamos a extraer y mostrar cada canal presente en cada uno de nuestros 8 mapas de características. Apilaremos los resultados en un gran tensor de imagen, con los canales colocados uno junto al otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NU3C1qdgdyQs"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Podemos pedir los nombres de las capas y usarlos para nuestro gráfico\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3283,
     "status": "ok",
     "timestamp": 1561417050807,
     "user": {
      "displayName": "Jorge Hermosillo",
      "photoUrl": "",
      "userId": "04400901820293359283"
     },
     "user_tz": 300
    },
    "id": "tI1_CVYgd2QW",
    "outputId": "01b94d7f-f2c3-4ac4-c01c-3ce346a31e16"
   },
   "outputs": [],
   "source": [
    "# Vamos a buscar desplegar nuestros mapas de características\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # Este es el número de características presentes en \n",
    "    # en un mapa de características\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # El mapa de características tiene la forma: (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Vamos a colocar los canales de activación en esta matriz\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # Colocaremos cada mapa en esta gran malla horizontal\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "            \n",
    "            # Procesaremos el mapa de características para que sea \n",
    "            # visualmente agradable\n",
    "            \n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Mostramos los mapas en la malla\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oh6Ip2qTYete"
   },
   "source": [
    "# Observaciones\n",
    "\n",
    "Del gráfico de mapas de características podemos notar lo siguiente:\n",
    "\n",
    "* La primera capa de la red actúa como una colección de varios detectores de borde. En esa etapa, las activaciones aún retienen casi toda la información presente en la imagen inicial.\n",
    "\n",
    "* A medida que avanzamos en profundidad, las activaciones se vuelven cada vez más abstractas y menos interpretables visualmente. Se comienzan a codificar conceptos de nivel superior como \"oreja de gato\" u \"ojo de gato\". Las representaciones superiores llevan cada vez menos información sobre el contenido visual de la imagen, y cada vez más información relacionada con la clase de la imagen.\n",
    "\n",
    "* La escasez de activaciones aumenta con la profundidad de la red: en la primera capa, todos los filtros se activan mediante la imagen de entrada, pero en las siguientes capas, más y más canales de activación están en blanco. Esto significa que el patrón codificado por el filtro no se encuentra en la imagen de entrada.\n",
    "\n",
    "# Comentarios Finales\n",
    "\n",
    "Acabamos de evidenciar un hecho muy importante de las representaciones aprendidas por las redes neuronales profundas: las características extraídas por una capa se vuelven cada vez más abstractas con la profundidad de la red. \n",
    "\n",
    "Las activaciones de las capas superiores contienen cada vez menos información sobre la entrada específica que se está viendo y más información sobre el objetivo (en el caso de este ejemplo, la clase de la imagen: gato o perro). Una red neuronal profunda actúa efectivamente como un *pipeline* (tubería) que destila la información, con datos en crudo que entran (en nuestro caso, imágenes RBG) y se transforman repetidamente de tal forma que la información irrelevante es filtrada (por ejemplo, la apariencia visual específica de la imagen) mientras que la información útil es magnificada y refinada (por ejemplo, la clase de la imagen).\n",
    "\n",
    "Esto es análogo a la forma en que los humanos y los animales perciben el mundo: después de observar una escena durante unos segundos, un humano puede recordar qué objetos abstractos estaban presentes en él (por ejemplo, una bicicleta, un árbol) pero muchas veces no puede recordar la apariencia específica de estos objetos.\n",
    "\n",
    "El cerebro ha aprendido a abstraer completamente la información visual, a transformarla en conceptos visuales de alto nivel mientras filtra por completo los detalles visuales irrelevantes, haciendo que sea tremendamente difícil recordar cómo se ven exactamente las cosas a nuestro alrededor.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Perros_Gatos.ipynb",
   "provenance": [
    {
     "file_id": "1Q3_K8I4_JGHLhz9ZwMD5AgrSlC9Sb3m9",
     "timestamp": 1561417132382
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
